{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dataframe for ham\n",
    "create a dataframe for spam\n",
    "get only necessary words removing punctuation, with lower case letters, from both spam and ham\n",
    "use all words to create a dictionary\n",
    "\n",
    "create a sparse matrix containing all the words\n",
    "attach each sparse matrix to each  target, and run classification\n",
    "train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTS OF LIBRARY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A DATAFRAME FOR HAM AND SPAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv')\n",
    "data.drop(columns=['Unnamed: 2',\"Unnamed: 3\", \"Unnamed: 4\"],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['v1']\n",
    "data = data['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels , test_size=0.2, random_state=42,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE FOR PREPROCESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = \"i loved that you loved it understood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(sentence):\n",
    "#     return word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_words(sentence):\n",
    "#     stemmed_words = []\n",
    "#     punctuations = ['!', '.', ',','?', ]\n",
    "#     for word in sentence:\n",
    "#         if word not in punctuations:\n",
    "#             stemmed_word = stemmer.stem(word)\n",
    "#             stemmed_words.append(stemmed_word)\n",
    "        \n",
    "#     return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok_sample = tokenize(sample)\n",
    "# tok_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.apply(tokenize)\n",
    "# x_train = x_train.apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1978    No I'm in the same boat. Still here at my moms...\n",
       "3989    (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "3935       They r giving a second chance to rahul dengra.\n",
       "4078       O i played smash bros  &lt;#&gt;  religiously.\n",
       "4086    PRIVATE! Your 2003 Account Statement for 07973...\n",
       "                              ...                        \n",
       "3772    I came hostel. I m going to sleep. Plz call me...\n",
       "5191                               Sorry, I'll call later\n",
       "5226        Prabha..i'm soryda..realy..frm heart i'm sory\n",
       "5390                           Nt joking seriously i told\n",
       "860                   In work now. Going have in few min.\n",
       "Name: v2, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = []\n",
    "# for message in x_train:\n",
    "#     for word in message:\n",
    "#         vocabulary.append(word)\n",
    "# vocabulary = set(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,7}\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processing_text(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,remove_punctuation= True, strip_email=True, to_lowercase= True, replace_url= True, replace_number = True, stemming = True):\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.strip_email = strip_email\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.replace_number = replace_number\n",
    "        self.replace_url = replace_url\n",
    "        self.stemming = stemming\n",
    "        self.stemmed_data = []\n",
    "        self.punctuations = ['!', '.', ',','?', ]\n",
    "    def fit(self , x, y= None,):\n",
    "        return self\n",
    "    def transform(self, x, y=None):\n",
    "        self.stemmed_data = []\n",
    "        for text in x:\n",
    "            tokenize_x = word_tokenize(text)\n",
    "            url_pattern = url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "            email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,7}\\b'\n",
    "            stemmed_sentence = []\n",
    "            for word in tokenize_x:\n",
    "                if self.remove_punctuation and word in self.punctuations:\n",
    "                        continue\n",
    "                \n",
    "                if self.replace_url and re.match(url_pattern,word):\n",
    "                    word = \"URL\"\n",
    "                         \n",
    "                if self.replace_number and re.match(r'\\+?[0-9][0-9\\-]+', word):\n",
    "                     word = \"NUMBER\"\n",
    "                     \n",
    "                if self.strip_email and re.match(email_pattern,word):\n",
    "                     continue    \n",
    "                if self.to_lowercase:\n",
    "                     word = word.lower()\n",
    "\n",
    "                if self.stemming:\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_sentence.append(stemmed_word)\n",
    "                else:\n",
    "                     stemmed_sentence.append(word)\n",
    "\n",
    "\n",
    "            self.stemmed_data.append(stemmed_sentence)\n",
    "\n",
    "        \n",
    "        \n",
    "        return self.stemmed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bag_of_words(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, top_words = 2000):\n",
    "        self.vocab = []\n",
    "        self.max_vocab = top_words\n",
    "        self.bag_of_words = []\n",
    "        self.top_words = top_words\n",
    "\n",
    "    def fit(self,X,y= None):\n",
    "        self.vocab = []\n",
    "        for text in X:\n",
    "            for word in text:\n",
    "                self.vocab.append(word)\n",
    "        unique_words = pd.Series(self.vocab).value_counts()\n",
    "        frequent_words = unique_words[0:self.top_words]\n",
    "        self.vocab = frequent_words\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        \n",
    "        for text in X:\n",
    "            bag = np.zeros(len(self.vocab),dtype=np.float32)\n",
    "            for ind, word in enumerate(self.vocab):\n",
    "                if word in text:\n",
    "                    bag[ind] = 1.0\n",
    "            self.bag_of_words.append(bag)\n",
    "        return self.bag_of_words       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    " ('pipe',processing_text()),\n",
    " ('bow',bag_of_words())\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_data = pipeline.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NDFrame.where() missing 1 required positional argument: 'cond'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[197], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mwhere(pipe_data[\u001b[39m0\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39m1.0\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: NDFrame.where() missing 1 required positional argument: 'cond'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam_ham_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
